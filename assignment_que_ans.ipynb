{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment-based Subjective Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo analyze the effect of categorical variables on the dependent variable, we can break it down into a few key steps. Typically, this is done \\nusing exploratory data analysis (EDA) and statistical techniques like cross-tabulation, chi-square tests, and visualizations \\n(such as bar charts or box plots) to determine relationships between the categorical variables and the dependent variable.\\n\\nHere\\'s a general approach to analyzing the effect of categorical variables:\\n\\n1. Cross-Tabulation\\n   - Method: For each categorical variable, create a cross-tabulation or contingency table to observe how the values of the categorical \\n    variables are distributed across the dependent variable categories.\\n   - Inference: This helps in identifying patterns or imbalances. For example, if you are looking at default status (dependent variable), you can see if certain categories (e.g., \"employment type\") have higher rates of default.\\n\\n### 2. Chi-Square Test of Independence\\n   - Method: Perform a chi-square test to assess if there is a statistically significant relationship between a categorical variable and the dependent variable.\\n   - Inference: If the test shows a significant result (p-value < 0.05), this suggests that the categorical variable might influence the dependent variable. For example, loan type could significantly affect the likelihood of default.\\n\\n### 3. Bar Charts and Count Plots\\n   - Method: Use bar charts or count plots to visualize the distribution of the dependent variable across different categories.\\n   - Inference: Visualizing the distribution can give a clear idea of the relationship. For instance, if a particular category (e.g., \"marital status\") leads to a significantly higher proportion of defaults, it indicates that the category may be a key driver of the dependent variable.\\n\\n### 4. Categorical Encoding (One-Hot or Label Encoding)\\n   - Method: Encode categorical variables and then use logistic regression or decision trees to assess the effect of these encoded variables on the dependent variable.\\n   - Inference: The importance of each categorical feature can be gauged from the model’s coefficients (in regression) or feature importance scores (in decision trees or random forests).\\n\\n### 5. Box Plots (for ordinal categories)\\n   - Method: For ordinal categorical variables, box plots can show how the dependent variable (if continuous) varies across different categories.\\n   - Inference: If there is a clear trend or difference in the distribution across categories, it suggests an influence on the dependent variable. For example, income brackets might show a clear difference in loan default likelihood.\\n\\n### Sample Inference:\\n- If you have education level as a categorical variable and loan default as the dependent variable, you might find that people with higher education levels tend to default less. This would indicate that education level has a negative correlation with loan default.\\n\\nThe exact inference depends on your dataset and the specific variables in question, but these are general approaches used to infer the effect of categorical variables on a dependent variable.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q1)From your analysis of the categorical variables from the dataset, what could you infer about \n",
    "# their effect on the dependent variable?       \n",
    "\n",
    "### ANSWER ####\n",
    "'''\n",
    "To analyze the effect of categorical variables on the dependent variable, we can break it down into a few key steps. Typically, this is done \n",
    "using exploratory data analysis (EDA) and statistical techniques like cross-tabulation, chi-square tests, and visualizations \n",
    "(such as bar charts or box plots) to determine relationships between the categorical variables and the dependent variable.\n",
    "\n",
    "Here's a general approach to analyzing the effect of categorical variables:\n",
    "\n",
    "1. Cross-Tabulation\n",
    "   - Method: For each categorical variable, create a cross-tabulation or contingency table to observe how the values of the categorical \n",
    "    variables are distributed across the dependent variable categories.\n",
    "   - Inference: This helps in identifying patterns or imbalances. For example, if you are looking at default status (dependent variable), you can see if certain categories (e.g., \"employment type\") have higher rates of default.\n",
    "\n",
    "### 2. Chi-Square Test of Independence\n",
    "   - Method: Perform a chi-square test to assess if there is a statistically significant relationship between a categorical variable and the dependent variable.\n",
    "   - Inference: If the test shows a significant result (p-value < 0.05), this suggests that the categorical variable might influence the dependent variable. For example, loan type could significantly affect the likelihood of default.\n",
    "\n",
    "### 3. Bar Charts and Count Plots\n",
    "   - Method: Use bar charts or count plots to visualize the distribution of the dependent variable across different categories.\n",
    "   - Inference: Visualizing the distribution can give a clear idea of the relationship. For instance, if a particular category (e.g., \"marital status\") leads to a significantly higher proportion of defaults, it indicates that the category may be a key driver of the dependent variable.\n",
    "\n",
    "### 4. Categorical Encoding (One-Hot or Label Encoding)\n",
    "   - Method: Encode categorical variables and then use logistic regression or decision trees to assess the effect of these encoded variables on the dependent variable.\n",
    "   - Inference: The importance of each categorical feature can be gauged from the model’s coefficients (in regression) or feature importance scores (in decision trees or random forests).\n",
    "\n",
    "### 5. Box Plots (for ordinal categories)\n",
    "   - Method: For ordinal categorical variables, box plots can show how the dependent variable (if continuous) varies across different categories.\n",
    "   - Inference: If there is a clear trend or difference in the distribution across categories, it suggests an influence on the dependent variable. For example, income brackets might show a clear difference in loan default likelihood.\n",
    "\n",
    "### Sample Inference:\n",
    "- If you have education level as a categorical variable and loan default as the dependent variable, you might find that people with higher education levels tend to default less. This would indicate that education level has a negative correlation with loan default.\n",
    "\n",
    "The exact inference depends on your dataset and the specific variables in question, but these are general approaches used to infer the effect of categorical variables on a dependent variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   color_blue  color_green  color_red\n",
      "0       False        False       True\n",
      "1        True        False      False\n",
      "2       False         True      False\n",
      "3        True        False      False\n",
      "4       False         True      False\n",
      "   color_green  color_red\n",
      "0        False       True\n",
      "1        False      False\n",
      "2         True      False\n",
      "3        False      False\n",
      "4         True      False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n|   | color_blue | color_green | color_red |\\n|---|------------|-------------|-----------|\\n| 0 | 0          | 0           | 1         |\\n| 1 | 1          | 0           | 0         |\\n| 2 | 0          | 1           | 0         |\\n| 3 | 1          | 0           | 0         |\\n| 4 | 0          | 1           | 0         |\\n\\n#Output with `drop_first=True`:\\n\\n|   | color_blue | color_green |\\n|---|------------|-------------|\\n| 0 | 0          | 0           |\\n| 1 | 1          | 0           |\\n| 2 | 0          | 1           |\\n| 3 | 1          | 0           |\\n| 4 | 0          | 1           |\\n\\n In this case, \"red\" is the reference category, and the remaining dummies represent how \"blue\" and \"green\" differ from \"red.\"\\n\\n ### Conclusion:\\n Using `drop_first=True` simplifies your model by avoiding redundant information and prevents multicollinearity, improving model \\n interpretability and efficiency.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2) Why is it important to use drop_first=True during dummy variable creation?\n",
    "### ANSWER ####\n",
    "\n",
    "'''\n",
    "Using `drop_first=True` in `get_dummies()` is important in dummy variable creation to prevent multicollinearity when performing \n",
    "regression analysis or machine learning algorithms. Here's why:\n",
    "\n",
    "1. Multicollinearity:\n",
    "   - When you have multiple categories for a categorical variable (e.g., \"red,\" \"blue,\" \"green\" for color), `get_dummies()` will create a \n",
    "   separate dummy variable for each category.\n",
    "   - If all categories are represented by dummy variables, one of the dummies can always be perfectly predicted by the others. This results \n",
    "   in perfect multicollinearity, where one variable is a linear combination of others.\n",
    "   - Multicollinearity can distort statistical tests and make it difficult to interpret the coefficients in regression models.\n",
    "\n",
    "2. Redundant Information:\n",
    "   - When you include all dummy variables, one is redundant. For example, if a categorical variable has three levels (A, B, C), \n",
    "   creating three dummies means if you know two of the dummy values, you can infer the third.\n",
    "   - Example: If a variable takes the values A, B, or C, and you create three dummy variables:\n",
    "     - A: [1, 0, 0]\n",
    "     - B: [0, 1, 0]\n",
    "     - C: [0, 0, 1]\n",
    "   - The third column can be inferred if you have the first two, leading to redundancy.\n",
    "\n",
    "3. drop_first=True:\n",
    "   - When `drop_first=True`, pandas drops the first dummy variable and only creates (k-1) dummy variables for k categories.\n",
    "   - This removes the redundancy and solves the multicollinearity issue.\n",
    "   - The dropped category is treated as a reference category, and the remaining dummies represent how the other categories differ \n",
    "   from that reference.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'blue', 'green']\n",
    "})\n",
    "\n",
    "# Without drop_first\n",
    "print(pd.get_dummies(df))\n",
    "\n",
    "# With drop_first\n",
    "print(pd.get_dummies(df, drop_first=True))\n",
    "\n",
    "#Output without `drop_first=True`:\n",
    "'''\n",
    "|   | color_blue | color_green | color_red |\n",
    "|---|------------|-------------|-----------|\n",
    "| 0 | 0          | 0           | 1         |\n",
    "| 1 | 1          | 0           | 0         |\n",
    "| 2 | 0          | 1           | 0         |\n",
    "| 3 | 1          | 0           | 0         |\n",
    "| 4 | 0          | 1           | 0         |\n",
    "\n",
    "#Output with `drop_first=True`:\n",
    "\n",
    "|   | color_blue | color_green |\n",
    "|---|------------|-------------|\n",
    "| 0 | 0          | 0           |\n",
    "| 1 | 1          | 0           |\n",
    "| 2 | 0          | 1           |\n",
    "| 3 | 1          | 0           |\n",
    "| 4 | 0          | 1           |\n",
    "\n",
    " In this case, \"red\" is the reference category, and the remaining dummies represent how \"blue\" and \"green\" differ from \"red.\"\n",
    "\n",
    " ### Conclusion:\n",
    " Using `drop_first=True` simplifies your model by avoiding redundant information and prevents multicollinearity, improving model \n",
    " interpretability and efficiency.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Based on the pair-plot you provided, the relationship between the variables can be observed visually. The target variable seems to be \\n \"cnt\" (likely representing a count of some event). From the scatter plots:\\n\\n- The variable **\"temp\" (temperature)** shows the strongest positive linear relationship with the target \"cnt.\" This can be seen from the \\ndiagonal pattern in the scatter plot between \"cnt\" and \"temp.\" \\n- Similarly, the variable \"atemp\" (which may represent apparent temperature) also shows a strong correlation with \"cnt.\"\\n\\nAmong these two, \"temp\" appears to have the strongest correlation with the target, based on visual inspection.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Looking at the pair-plot among the numerical variables, which one has the highest correlation \n",
    "#with the target variable? \n",
    "# ANswer\n",
    "'''\n",
    " Based on the pair-plot you provided, the relationship between the variables can be observed visually. The target variable seems to be \n",
    " \"cnt\" (likely representing a count of some event). From the scatter plots:\n",
    "\n",
    "- The variable \"temp\" (temperature) shows the strongest positive linear relationship with the target \"cnt.\" This can be seen from the \n",
    "diagonal pattern in the scatter plot between \"cnt\" and \"temp.\" \n",
    "- Similarly, the variable \"atemp\" (which may represent apparent temperature) also shows a strong correlation with \"cnt.\"\n",
    "\n",
    "Among these two, \"temp\" appears to have the strongest correlation with the target, based on visual inspection.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5) How did you validate the assumptions of Linear Regression after building the model on the \n",
    "# training set?\n",
    "\n",
    "### ANSWER \n",
    "'''\n",
    "After building a Linear Regression model, its crucial to validate the assumptions of the model to ensure its accuracy and generalizability. The following are the key assumptions of Linear Regression and common techniques to validate them:\n",
    "\n",
    "### 1. Linearity of the relationship between features and target:\n",
    "   - Assumption: The dependent variable (target) should have a linear relationship with each independent variable (features).\n",
    "   - How to validate:\n",
    "     - Residual Plot: Plot residuals (difference between observed and predicted values) versus the predicted values. The residuals should be randomly scattered around zero, without any distinct patterns (e.g., curved or funnel-shaped).\n",
    "     - Scatter Plot: Plot the features against the target variable to visually check for linear relationships.\n",
    "     - Partial Regression Plots: These help to visualize the effect of each predictor on the target while keeping other variables constant.\n",
    "   \n",
    "   Corrective Action: Apply transformations (e.g., log, polynomial features) if the relationships are non-linear.\n",
    "\n",
    "### 2. Homoscedasticity (constant variance of errors):\n",
    "   - Assumption: The variance of the residuals should remain constant across all levels of predicted values.\n",
    "   - How to validate:\n",
    "     - Residual Plot: Look for patterns in the residual plot. If the residuals show a \"funnel\" shape (i.e., the variance increases or decreases as the predicted values increase), it indicates heteroscedasticity.\n",
    "   \n",
    "   Corrective Action: Apply transformations to the dependent variable (e.g., log transformation) or use models that can handle heteroscedasticity (e.g., Generalized Least Squares).\n",
    "\n",
    "### 3. Independence of errors:\n",
    "   - Assumption: The residuals (errors) should be independent of each other, meaning there is no autocorrelation.\n",
    "   - How to validate:\n",
    "     - Durbin-Watson Test: This statistical test detects the presence of autocorrelation in the residuals. A value close to 2 indicates no autocorrelation, while values close to 0 or 4 suggest positive or negative autocorrelation, respectively.\n",
    "     - Plot Residuals over Time: If your data is time-based (e.g., time series), plot the residuals over time to detect any patterns (which indicate dependence).\n",
    "   \n",
    "   Corrective Action: If autocorrelation is present, you might need to use time-series-specific techniques such as ARIMA models.\n",
    "\n",
    "### 4. Normality of residuals:\n",
    "   - Assumption: The residuals should be normally distributed.\n",
    "   - How to validate:\n",
    "     - Histogram or Q-Q Plot: Plot a histogram of the residuals or use a Q-Q plot (Quantile-Quantile plot) to check if the residuals follow a normal distribution. In a Q-Q plot, the points should fall along the 45-degree reference line if the residuals are normally distributed.\n",
    "     - Shapiro-Wilk Test: A formal statistical test for normality. However, this test can be overly sensitive in large datasets, so visual inspections are often more practical.\n",
    "   \n",
    "   Corrective Action: If the residuals are not normally distributed, you may need to apply transformations to the target variable (e.g., log transformation).\n",
    "\n",
    "### 5. No multicollinearity among independent variables:\n",
    "   - Assumption: The independent variables should not be highly correlated with each other.\n",
    "   - How to validate:\n",
    "     - Variance Inflation Factor (VIF): Calculate VIF for each independent variable. A VIF > 10 suggests a high level of multicollinearity.\n",
    "     - Correlation Matrix: Check the pairwise correlation among features. High correlation values (e.g., > 0.8 or < -0.8) between two or more features indicate multicollinearity.\n",
    "   \n",
    "   Corrective Action: Remove or combine highly correlated features, or use techniques like Principal Component Analysis (PCA) or Ridge Regression to handle multicollinearity.\n",
    "\n",
    "### 6. Outliers and Influential Points:\n",
    "   - Assumption: Outliers and high-leverage points (influential observations) can distort the model’s performance.\n",
    "   - How to validate:\n",
    "     - Leverage vs. Residuals Plot: Check for points that have high leverage and large residuals.\n",
    "     - Cook’s Distance: This statistic identifies influential points. Values greater than 4/n (where n is the number of data points) are typically considered influential.\n",
    "     - Boxplots: Use boxplots to visually inspect outliers in the independent variables.\n",
    "   \n",
    "   Corrective Action: Investigate and, if appropriate, remove or transform the outliers. Alternatively, you can use robust regression techniques that are less sensitive to outliers.\n",
    "\n",
    "### 7. No Omitted Variable Bias:\n",
    "   - Assumption: All relevant variables are included in the model.\n",
    "   - How to validate:\n",
    "     - Domain Knowledge: Ensure that you include all significant predictors based on your understanding of the problem.\n",
    "     - Comparison of Models: Compare the performance of different models, with and without suspected omitted variables, to see if their inclusion significantly improves performance.\n",
    "\n",
    "### Conclusion:\n",
    "Validating these assumptions ensures that your Linear Regression model is reliable and generalizes well to unseen data. If any assumptions are violated, applying the appropriate transformations or using alternative models will help improve your model's robustness and accuracy.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvariables year , season/ weather situation and month are significant in predicting the demand for shared bikes .\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on the final model, which are the top 3 features contributing significantly towards \n",
    "# explaining the demand of the shared bikes?\n",
    "\n",
    "# ANSWER \n",
    "'''\n",
    "variables year , season/ weather situation and month are significant in predicting the demand for shared bikes .\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "upgrad_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
